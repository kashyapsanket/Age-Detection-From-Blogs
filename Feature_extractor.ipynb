{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_extractor",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashyapsanket/Age-Detection-From-Blogs/blob/master/Feature_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ddEAhf-SnU6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kIMb5Ek8nfyC",
        "colab_type": "code",
        "outputId": "a9a34979-3b69-4f0e-ebcf-e888922d94a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYdzfmiinwz5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "POS_dic = {\"CC\":1, \"CD\":2, \"DT\":3, \"EX\":4, \"FW\":5, \"IN\":6, \"JJ\":7, \"JJR\":8, \"JJS\":9, \"LS\":10, \"MD\":11, \"NN\":12, \"NNS\":13, \"NNP\":14, \"NNPS\":15, \"PDT\":16, \"POS\":17,\\\n",
        "\"PRP\":18, \"PRP$\":19, \"RB\":20, \"RBR\":21, \"RBS\":22, \"RP\":23, \"SYM\":24, \"TO\":25, \"UH\":26, \"VB\":27, \"VBD\":28, \"VBG\":29, \"VBN\":30, \"VBP\":31, \"VBZ\":32, \"WDT\":33, \"WP\":34, \"WP$\":35, \"WRB\":36}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3BpS-LM1n7Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sentence_information(text):\n",
        "    #the list of feature to be returned\n",
        "    # #4 features\n",
        "    text = re.sub(\"!{2,}\",\"!\", text)\n",
        "    text = re.sub(\"\\.{2,}\",\".\", text)\n",
        "    text = re.sub(\"\\?{2,}\",\"?\", text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text) #tokenise the string\n",
        "    to_return = []\n",
        "\n",
        "    split = ['.','?', '!']\n",
        "    lower_count = 0\n",
        "    upper_count = 0\n",
        "    current_sentence_count = 0\n",
        "    sentence_count = 0\n",
        "    word_count = 0\n",
        "\n",
        "    punc =['.',',','?','!',';',':']\n",
        "    for token in tokens:\n",
        "        if token in split:\n",
        "            if current_sentence_count > 0:\n",
        "                sentence_count += 1\n",
        "            current_sentence_count = 0\n",
        "        elif token not in punc:\n",
        "            current_sentence_count+=1\n",
        "            if current_sentence_count == 1:\n",
        "                if token[0].isupper():\n",
        "                    upper_count+=1\n",
        "                else:\n",
        "                    lower_count+=1\n",
        "                word_count+=1\n",
        "\n",
        "    if sentence_count == 0:\n",
        "        sentence_count = 1\n",
        "    to_return.append(sentence_count) #total no of sentences\n",
        "    to_return.append(float(word_count)/sentence_count) #average word count of sentences\n",
        "    to_return.append(float(upper_count)/sentence_count) # % of sentences with upper case starting\n",
        "    to_return.append(float(lower_count)/sentence_count) # % of sentences with lower case starting\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB_rFWtsn9CW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_lines_information(data_string):\n",
        "    #the list of features to be returned\n",
        "    # #4 features\n",
        "    #print(\"Line wise\")\n",
        "    to_return = []\n",
        "    to_return.append(data_string.count(\"\\n\")) #count the number of line\n",
        "    line_data = data_string.split(\"\\n\")\n",
        "    punc =['.',',','?','!',';',':']\n",
        "    char_count = 0\n",
        "    word_count = 0\n",
        "    empty_count = 0\n",
        "    for i in range(len(line_data)):\n",
        "        if line_data[i] == \"\":\n",
        "            empty_count+=1\n",
        "        else:\n",
        "            char_count+=len(line_data[i])\n",
        "            tokens = nltk.word_tokenize(line_data[i])\n",
        "            for j in tokens:\n",
        "                if j not in punc:\n",
        "                    word_count+=1\n",
        "    to_return.append(float(empty_count)/to_return[0]) #count the % of empty lines\n",
        "    to_return.append(float(char_count)/to_return[0]) #count the average no of characters in a line\n",
        "    to_return.append(float(word_count)/to_return[0]) #count the average no fo words in a line\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4v5_VmcoBOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def character_features(data_string):\n",
        "    #the list to be returned\n",
        "    # #7 features\n",
        "    to_return = []\n",
        "    to_return.append(len(data_string)) #total no of characters\n",
        "    to_return.append(float(sum(c.isalpha() and c.islower() for c in data_string))/to_return[0]) # total no of letters\n",
        "    to_return.append(float(sum(c.isdigit() for c in data_string))/to_return[0]) # total no of digital characters\n",
        "    to_return.append(float(sum(c.isupper() for c in data_string))/to_return[0]) #total no of upper case characters\n",
        "    to_return.append(float(sum(c.isspace() for c in data_string))/to_return[0]) #total no of whitespace characters\n",
        "    to_return.append(float(data_string.count('\\t'))/to_return[0]) # total no of tab space characters\n",
        "    to_return.append(float(1-to_return[1]-to_return[2]-to_return[3]-to_return[4]-to_return[5])) # total no of special characters\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzMzC5O2oHRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def function_words_features(data_string):\n",
        "    # total 7 features\n",
        "    tokened_data_string = nltk.word_tokenize(data_string)\n",
        "    total_words = len(tokened_data_string)\n",
        "    if total_words == 0:\n",
        "        return [0, 0, 0, 0, 0, 0, 0]\n",
        "    to_return = []\n",
        "    postagtext = nltk.pos_tag(tokened_data_string)\n",
        "    to_return.append(float(str(postagtext).count('DT'))/float(total_words))\n",
        "    to_return.append(float((data_string.lower().count('yes') + data_string.lower().count('no') + data_string.lower().count('okay') + data_string.lower().count('ok')))/float(total_words))\n",
        "    total_pronouns = str(postagtext).count('PP') + str(postagtext).count('PP$') + str(postagtext).count('WP') + str(postagtext).count('WP$')\n",
        "    to_return.append(float(total_pronouns)/float((total_words)))\n",
        "    to_return.append(float((str(postagtext).count('VB') + str(postagtext).count('VBD') + str(postagtext).count('VBG') + str(postagtext).count('VBN') + str(postagtext).count('VBP') + str(postagtext).count('VBZ'))) / float(total_words))\n",
        "    to_return.append(float(str(postagtext).count('CC'))/float(total_words))\n",
        "    to_return.append(float(str(postagtext).count('UH'))/float(total_words))\n",
        "    to_return.append(float(str(postagtext).count('IN'))/float(total_words))\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-jVG0RZyoNUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_simpson_d_measure(data_word_count, no_of_words):\n",
        "    to_return = 0\n",
        "    for word_count in data_word_count.keys():\n",
        "        if no_of_words == 1:\n",
        "            continue\n",
        "        to_return = to_return + float(len(data_word_count[word_count])*float(word_count)*float(word_count-1)/float(no_of_words*(no_of_words-1)))\n",
        "    return to_return\n",
        "\n",
        "def calculate_sichel_s_measure(di_hapaxs,word_set_len):\n",
        "    to_return = 0\n",
        "    if word_set_len == 0:\n",
        "        return 0\n",
        "    to_return = to_return + float(float(len(di_hapaxs))/word_set_len)\n",
        "    return to_return\n",
        "\n",
        "def calculate_honore_r_measure(hapaxs,word_set_len,no_of_words):\n",
        "    to_return = 0\n",
        "    to_return = to_return + float(100*math.log10(no_of_words))\n",
        "    if word_set_len == 0:\n",
        "        return to_return\n",
        "    to_return = to_return/float(1-float(len(hapaxs))/word_set_len)\n",
        "    return to_return\n",
        "\n",
        "def calculate_entropy_measure(data_word_count,no_of_words):\n",
        "    to_return = 0\n",
        "    for word_count in data_word_count.keys():\n",
        "        to_return = to_return + float(len(data_word_count[word_count])*math.log10(float(no_of_words)/word_count)*float(word_count)/no_of_words)\n",
        "    return to_return\n",
        "  \n",
        "def calculate_yule_k(data_word_count,no_of_words):\n",
        "    to_return = 0\n",
        "    to_return = to_return + 10000.0/float(no_of_words)\n",
        "    for word_count in data_word_count.keys():\n",
        "        to_return = to_return + float(10000.0*(float(len(data_word_count[word_count]))*(float(word_count/no_of_words))**2))\n",
        "    return to_return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1D5deXcxoSXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_based_features(data_string):\n",
        "    #the list of features to be returned\n",
        "    # #currently 32 features\n",
        "    punctuations = ['.',',','?','!',';',':']\n",
        "    to_return = []\n",
        "    word_len_count = [0]*20\n",
        "    data_tokens = nltk.word_tokenize(data_string.lower()) #tokenise the string\n",
        "    data_tokens = [i for i in data_tokens if i not in punctuations and i != '']\n",
        "    if len(data_tokens) == 0:\n",
        "        return [0]*31\n",
        "    data_tokens_set = set(data_tokens)\n",
        "    data_tokens_len = [float(len(i)) + 1 for i in data_tokens] #get tokens length\n",
        "    data_tokens_larger_6 = [i for i in data_tokens if len(i)>6] #get tokens of length > 6\n",
        "    data_tokens_small = [i for i in data_tokens if len(i) < 4] #get tokens of length < 4\n",
        "    data_count = Counter(data_tokens)\n",
        "    data_count_dict = dict(data_count) #get count of each token\n",
        "    data_word_count = {}\n",
        "    for i in data_count_dict.keys():\n",
        "        try:\n",
        "            data_word_count[data_count_dict[i]].append(i)\n",
        "        except KeyError:\n",
        "            data_word_count[data_count_dict[i]] = []\n",
        "            data_word_count[data_count_dict[i]].append(i)\n",
        "    hapaxs = [i for i in data_count_dict.keys() if data_count_dict[i] == 1] \n",
        "    di_hapaxs = [i for i in data_count_dict.keys() if data_count_dict[i] == 2]\n",
        "    to_return.append(len(data_tokens)) #no of words\n",
        "    to_return.append(sum(data_tokens_len)/float(len(data_tokens_len))) #average length of words\n",
        "    to_return.append(float(len(set(data_tokens)))/float(len(data_tokens))) #no of distinct words, vocabulary strength\n",
        "    to_return.append(float(len(data_tokens_larger_6))/float(len(data_tokens))) #no of words with length > 6\n",
        "    to_return.append(float(len(data_tokens_small))/float(len(data_tokens))) #no of words with length < 4\n",
        "    to_return.append(float(len(hapaxs))/float(len(data_tokens))) #hapax legomena\n",
        "    to_return.append(float(len(di_hapaxs))/float(len(data_tokens))) #hapax dislegomena\n",
        "    to_return.append(calculate_yule_k(data_word_count,len(data_tokens))) #yule's k measure\n",
        "    to_return.append(calculate_simpson_d_measure(data_word_count,len(data_tokens))) #simpsons d measure\n",
        "    to_return.append(calculate_sichel_s_measure(di_hapaxs,len(data_tokens_set))) #sichels s measures\n",
        "    to_return.append(calculate_entropy_measure(data_word_count,len(data_tokens))) #calculate the entropy measure\n",
        "    for i in data_tokens:\n",
        "        try:\n",
        "            word_len_count[len(i)]+=1\n",
        "        except:\n",
        "            pass\n",
        "    to_return.extend(word_len_count) #20 features\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OwjH_yUboXLw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pos_tag_start(data_string):\n",
        "    global POS_dic\n",
        "    to_return = [0]*36\n",
        "    tokened_data_string = nltk.word_tokenize(data_string)\n",
        "    postagtext = nltk.pos_tag(tokened_data_string)\n",
        "    punc =['.','?',';']\n",
        "    sen = []\n",
        "    sen_list1=[]\n",
        "    for i in postagtext:\n",
        "        if not i[0] in punc:\n",
        "            sen.append(i)\n",
        "        else:\n",
        "            sen_list1.append(sen)\n",
        "            sen=[]\n",
        "    while [] in sen_list1:\n",
        "        sen_list1.remove([])\n",
        "    for i in sen_list1:\n",
        "        try:\n",
        "            to_return[POS_dic[i[0][1]]-1]+=1\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return to_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKoRyytrobry",
        "colab_type": "code",
        "outputId": "94de4235-9f48-4c28-eca4-dbd9230d308c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')\n",
        " nltk.download('averaged_perceptron_tagger')\n",
        " df = pd.read_csv('gdrive/My Drive/BlogData.csv')\n",
        " text_df = df[['Text', 'Age']]\n",
        " #print(text_df)\n",
        " temp_df = text_df[0:100000]\n",
        " feature_set = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZROpyvEo4Dk",
        "colab_type": "code",
        "outputId": "015051ca-464c-483c-d809-1687ac55c570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "for index, row in temp_df.iterrows():\n",
        "  features = []\n",
        "  features.extend(pos_tag_start(row['Text']))\n",
        "  features.extend(get_sentence_information(row['Text']))\n",
        "  features.extend(get_lines_information(row['Text']))\n",
        "  features.extend(word_based_features(row['Text']))\n",
        "  features.extend(function_words_features(row['Text']))\n",
        "  features.extend(character_features(row['Text']))\n",
        "  feature_set.append(features)\n",
        "  if index%10000 == 0:\n",
        "    print(index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRfzD4dGpzNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feature_df = pd.DataFrame(feature_set)\n",
        "feature_df.to_csv('normalizedfeatures.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1j3sabOp_4z",
        "colab_type": "code",
        "outputId": "ea497d2d-0d73-4284-85ab-0ec3a14ddc04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       1\n",
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "5       0\n",
              "6       0\n",
              "7       6\n",
              "8       0\n",
              "9       1\n",
              "10      0\n",
              "11      1\n",
              "12      0\n",
              "13      0\n",
              "14      1\n",
              "15      7\n",
              "16      3\n",
              "17      0\n",
              "18      2\n",
              "19      0\n",
              "20      4\n",
              "21      7\n",
              "22      3\n",
              "23      4\n",
              "24      0\n",
              "25      1\n",
              "26      6\n",
              "27      3\n",
              "28      2\n",
              "29      7\n",
              "       ..\n",
              "9969    0\n",
              "9970    0\n",
              "9971    2\n",
              "9972    0\n",
              "9973    0\n",
              "9974    0\n",
              "9975    0\n",
              "9976    0\n",
              "9977    0\n",
              "9978    0\n",
              "9979    0\n",
              "9980    0\n",
              "9981    0\n",
              "9982    0\n",
              "9983    0\n",
              "9984    0\n",
              "9985    0\n",
              "9986    4\n",
              "9987    4\n",
              "9988    1\n",
              "9989    5\n",
              "9990    0\n",
              "9991    0\n",
              "9992    0\n",
              "9993    1\n",
              "9994    0\n",
              "9995    1\n",
              "9996    1\n",
              "9997    0\n",
              "9998    1\n",
              "Name: 0, Length: 9999, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}